---
title: "Process Imputation of Animal Telemetry Data"
author: "Josh Cullen"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  pdf_document: 
    fig_caption: yes
    latex_engine: xelatex
urlcolor: blue
header-includes:
  - \usepackage{float}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

# Imputation Approaches

The snail kite telemetry data, as well as many telemetry datasets, are sampled irregularly in time. While this would not be a problem in some analyses (point process models, continuous-time movement models), the irregular sampling frequency precludes the consistent measurement of movement parameters (step length, turning angle, first passage time, directional persistence, etc) for discrete-time movement models. One possible solution would be to remove all observations that are not sampled at the time step of interest, although this can result in large gaps within the time series. Another simple solution is the use of linear interpolation (i.e. connecting the dots), which assumes a constant speed across consecutive observations and estimates these missing data on a linear path between true observations. This method (1) provides a biased view of the latent trajectory because it is not based on a mechanistic process of the movement dynamics, (2) the observed data are assumed to represent completely accurate positions of the latent trajectory, and (3) uncertainty in the trajectory is not accounted for at the observations or missing data. A more robust method that can account for these limitations is multiple imputation.
\hfill\break

Multiple imputation is an iterative form of stochastic imputation, which generates a distribution from which inference can be made on the missing values. With respect to animal telemetry data, this includes the application of a process model (e.g. continuous-time correlated random walk using an Ornstein-Uhlenbeck process) as a representation of the latent trajectory to impute missing locations multiple times. This process accounts for the assumed underlying movement dynamics as well as measurement error of the location estimates. Multiple imputation depends on the ability to evaluate the complete-data posterior (i.e. model parameters given the observed and missing data), and the ability to sample missing data sets from the imputation distribution (i.e. the missing data given the observed data). This can be written as:

$$\begin{aligned} \left[ \boldsymbol{\theta }| \mathbf {s}\right]&= \int \left[ \boldsymbol{\theta }, \mathbf {s}_m | \mathbf {s}\right] d\mathbf {s}_m, \\
&= \int \left[ \boldsymbol{\theta }| \mathbf {s}, \mathbf {s}_m \right] \left[ \mathbf {s}_m | \mathbf {s}\right] d\mathbf {s}_m,\end{aligned}$$

where $\boldsymbol{\theta}$ is a vector of the model parameters, $\mathbf{s}$ is a vector of the data, and $\mathbf{s}_m$ is a vector representing the missing data. In this case, $\left[ \boldsymbol{\theta }| \mathbf {s}\right]$ represents the desired posterior distribution, $\left[ \mathbf {s}_m | \mathbf {s}\right]$ represents the imputation distribution, and $\left[ \boldsymbol{\theta }| \mathbf {s}, \mathbf {s}_m \right]$ represents the complete-data posterior distribution. If $\mathit{K}$ samples are drawn from the imputation distribution, the posterior expectation $\mathbb{E} (\boldsymbol{\theta} | \mathbf{s})$ can be approximated by averaging the imputed data $\mathbf{s}_m$ across all draws. By similarly averaging the variances across all $\mathit{K}$ draws, these can provide a close approximation of the true posterior variance $\text{Var} (\boldsymbol{\theta} | \mathbf{s})$.
\hfill\break

In terms of animal movement analyses, the telemetry data are defined as $\mathbf {s}\equiv (\mathbf {s}(t_1)',\ldots ,\mathbf {s}(t_n)')'$, a $2n \times 1$ vector, for observation times $t_1, \ldots , t_n$. The true latent position process is represented as $\boldsymbol{\mu }\equiv \left( \boldsymbol{\mu }(t_1)',\ldots ,\boldsymbol{\mu }(t_m)' \right) '$, a $2m \times 1$ vector, where each $\boldsymbol{\mu }(t_j)$ represents a true, but unknown position at time $t_j$. The true position process is often modeled as a distribution conditioned on a set of model parameters $\boldsymbol{\theta}$, such that $\boldsymbol{\mu } \sim \left[\boldsymbol{\mu} | \boldsymbol{\theta} \right]$. In a hierarchical framework, the observed telemetry data are conditioned on the latent trajectory as well as the observation parameters $\boldsymbol{\psi}$, such that $\mathbf {s}\sim \left[ \mathbf {s}| \boldsymbol{\mu }, \boldsymbol{\psi }\right]$. In some circumstances however, fitting a hierchical model of animal telemetry data can exceed computational resources, especially when applying an MCMC algorithm. A method proposed by [Scharf et al. (2017)](https://link.springer.com/article/10.1007/s13253-017-0294-5) can be used to avoid this computational burden using an approximation method termed "process imputation".


# Process Imputation

Process imputation is proposed as a method that is "similar in spirit" to multiple imputation, but applies an approximate model-fitting procedure that was originally developed by [Hooten et al. (2010)](https://link.springer.com/article/10.1007%2Fs13253-010-0038-2) and [Hanks et al. (2011)](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0022795). The motivation for this approach arises from a decomposition of the posterior distribution of the process parameters. Using standard properties of conditional probability, the posterior distribution is written as:

$$\begin{aligned} \left[ \boldsymbol{\theta }| \mathbf {s}\right]&= \int \left[ \boldsymbol{\theta }| \boldsymbol{\mu }, \mathbf {s}\right] \left[ \boldsymbol{\mu }| \mathbf {s}\right] d\boldsymbol{\mu }. \end{aligned}$$

This form is similar to that of the first integral which showed an approach for multiple imputation, except that $\mathbf{s}_m$ is replaced by $\boldsymbol{\mu}$. While evaluating the posterior distribution $\left[ \boldsymbol{\theta }| \boldsymbol{\mu }, \mathbf {s}\right]$ up to a proportionality is relatively straightforward, it is more challenging to sample from the process imputation distribution $\left[ \boldsymbol{\mu }| \mathbf {s}\right]$. This is because sampling from the process imputation distribution $\left[ \boldsymbol{\mu }| \mathbf {s}\right] \propto \int \left[ \mathbf {s}| \boldsymbol{\mu }, \boldsymbol{\psi }\right] \left[ \boldsymbol{\mu }\right] \left[ \boldsymbol{\psi }\right] d\boldsymbol{\psi }$ requires that we have the marginal distribution $[\boldsymbol{\mu}]$, which also requires that we evaluate $\left[ \boldsymbol{\mu }\right] = \int \left[ \boldsymbol{\mu }| \boldsymbol{\theta }\right] \left[ \boldsymbol{\theta }\right] d\boldsymbol{\theta }$. For animal telemetry models, this integral is intractable as a result of either noninvertibility or computational burden.
\hfill\break

To avoid the issues of sampling from the process imputation distribution $\left[ \boldsymbol{\mu }| \mathbf {s}\right]$, we can instead sample from another conditional parameter $\boldsymbol{\mu}^*$. We work under the assumption that $\boldsymbol{\mu}^*$ is sufficiently similar to $\boldsymbol{\mu}$
that draws from $\left[ \boldsymbol{\mu}^*| \mathbf {s}\right]$ can be used to approximate $\begin{aligned} \left[ \boldsymbol{\theta }| \mathbf {s}\right]&= \int \left[ \boldsymbol{\theta }| \boldsymbol{\mu }, \mathbf {s}\right] \left[ \boldsymbol{\mu }| \mathbf {s}\right] d\boldsymbol{\mu }. \end{aligned}$ Scharf et al. (2017) refer to $\left[ \boldsymbol{\mu}^*| \mathbf {s}\right]$ as the approximate imputation distribution (AID), which is used to discern the true posterior distribution $[\boldsymbol{\theta}^* | \mathbf{s}]$. To perform process imputation, you must first specify a model for $\left[ \boldsymbol{\mu }^* | \mathbf {s}, \boldsymbol{\phi }\right]$, which is parameterized by $\boldsymbol{\phi}$. Estimates of $\boldsymbol{\phi}$ are intially determined by fitting $\left[ \boldsymbol{\mu }^* | \mathbf {s}, \boldsymbol{\phi }\right]$ to the data. While multiple models can be used to fit the AID, the one used in this demonstration is the Ornstein--Uhlenbeck velocity process ([Johnson et al., 2008](https://esajournals.onlinelibrary.wiley.com/doi/full/10.1890/07-1032.1)). Additional details regarding process imputation can be found in the publication by Scharf et al. (2017) and a vignette in R for fitting the model can be found in the Supplementary Material.
\hfill\break

# Example of Process Imputation of Snail Kite Data

In this example, I will be using the *crawl* package in R to fit the process imputation model based on a Ornstein-Uhlenbeck velocity process within a continuous-time correlated random walk model (CTCRW). Within this framework, I will include an observation error of 30 m in both the x and y directions for each location estimate at a regular time interval of 1 h. Upon fitting the CTCRW model, I will draw 20 samples from the AID to visualize the variability of the trajectory for each of the IDs. From this AID, I will calculate the mean position for the missing locations on the time interval of 1 h, which can be used in all further analyses for deriving movement parameters. These will be used to segment time of the time series for the movement parameters and the clustering of these segments for behavior classification. 

```{r load libraries and data}

library(tidyverse)
library(lubridate)
library(rgdal)
library(crawl)
library(sf)
library(furrr)
library(ggspatial)
library(cowplot)
library(adehabitatLT)

source('helper functions.R')

dat<- read.csv("Snail Kite Gridded Data_large.csv", as.is = T)
dat$ESTtime<- as_datetime(dat$ESTtime)
dat_red<- dat %>% dplyr::select(id, ESTtime, utmlong, utmlat) #%>% filter(id != 28)
dat_red <- dat_red %>%
  mutate(
    error_semi_major_axis = 30,
    error_semi_minor_axis = 30,
    error_ellipse_orientation = 0
  )

sf_locs <- st_as_sf(dat_red, coords = c("utmlong","utmlat")) %>% st_set_crs(32617)

future::plan(multisession)
sf_locs <- sf_locs %>% 
  dplyr::group_by(id) %>% dplyr::arrange(ESTtime) %>% 
  tidyr::nest() %>% 
  dplyr::mutate(data = furrr::future_map(data,sf::st_as_sf))
#############################################################################
### STAGE 1: Ornstein-Uhlenbeck Approximate Imputation Distribution (AID) ###
#############################################################################
set.seed(123)

#set initial values for model
initial<- map(sf_locs[[2]], init_params)


# Set first val of 'fixpar' to 1 since we are providing error info; second and third are set to NA (for sigma and beta, respectively) to be estimated from model; sigma = velocity variation; beta = velocity autocorr
sf_locs <- sf_locs %>% 
  dplyr::mutate(fixpar = rep(list(c(1,NA,NA)), nrow(.)))


# Run crawl model on all IDs
dat_fit <- sf_locs %>% 
  dplyr::mutate(fit = furrr::future_pmap(list(d = data,fixpar = fixpar),
                                         fit_crawl, .progress = FALSE),
                params = map(fit, crawl::tidy_crwFit))
## Draw from AID
set.seed(1)

K <- 20  #number of draws

#includes observed and predicted locs
dat_fit <- dat_fit %>% 
  dplyr::mutate(sim_tracks = furrr::future_map(fit, .get_sim_tracks, iter = K, .progress = FALSE))

#filtering for only predicted locs and creating sf objects
dat_fit <- dat_fit %>% 
  dplyr::mutate(sim_lines = crawl::crw_as_sf(.$sim_tracks, ftype = "MULTILINESTRING",
                                             locType = "p"))
```

```{r extract coords}

sf_sim_lines <- do.call(rbind,dat_fit$sim_lines) %>% mutate(id = dat_fit$id)

# Create list of mean locations by ID from imputed locs only
sim_tracks<- modify_depth(dat_fit$sim_tracks, 2,
                          function(x) cbind(x$alpha.sim[x$locType == "p", c("mu.x","mu.y")],
                                            time = x$ESTtime[x$locType == "p"])) %>% 
  lapply(., function(x) do.call(cbind, x)) %>%
  lapply(., function(x) cbind(mu.x = apply(x[,which(colnames(x) == "mu.x")], 1, mean),
                              mu.y = apply(x[,which(colnames(x) == "mu.y")], 1, mean),
                              sd.x = apply(x[,which(colnames(x) == "mu.x")], 1, sd),
                              sd.y = apply(x[,which(colnames(x) == "mu.y")], 1, sd),
                              time = x[,"time"])) %>%
  map(., data.frame) %>% 
  set_names(dat_fit$id)

#change from numeric to POSIXct
sim_tracks<- lapply(sim_tracks, function(x){x$time <- intToPOSIX(x$time, tz = "UTC"); x})

#make single DF and add 'id' column
id.vec<- rep(names(sim_tracks), lapply(sim_tracks, nrow) %>% unlist())
sim_tracks_df<- map_dfr(sim_tracks, `[`) %>% cbind(id = id.vec, .)
```

```{r plot, fig.align='center', fig.width=12, fig.height=9, fig.pos='H', fig.cap="Simulated tracks are shown in different colors by ID for all 20 draws from the AID. Dark grey points represent known observations at irregular time intervals, while light grey points represent at a regular time interval of 1 h."}
ggplot(data = sf_sim_lines) +
  geom_sf(aes(color = as.factor(id)), size = 0.25, alpha = 1, show.legend = "line") +
  geom_point(data = sim_tracks_df, aes(mu.x, mu.y), color = "grey80", alpha = 0.6, size = 1) +
  geom_point(data = dat, aes(utmlong, utmlat), color = "grey45", alpha = 0.6) +
  theme_bw() +
  facet_wrap(~id) +
  scale_color_discrete("ID", guide = guide_legend(override.aes = list(size = 1))) +
  labs(x="Longitude", y="Latitude")
```
\hfill\break

# Comparison of Observed vs Imputed Data

Given occasional large time gaps in the data for all IDs, it is necessary to evaluate the proportion of imputed vs observed data. This is ensures that statistical inference is primarily made on the observed data instead of on predicted data that may not be representative of the true latent trajectory. To quickly evaluate this, two plots are shown below that display the proportion of observed or imputed data based upon the maximum time interval used to perform process imputation (1, 2, 12, 24 hrs). The total number of observations by ID ($N$) is shown for reference. While there is a relatively wide range in the proportion of observed/imputed data relative to $N$, it appears that the time interval with the greatest boost in observed data and lowest proportion of imputed data occurs for the **2 hr time interval**. However, this would still result in the loss of data that are not observed at 1 or 2 hr time intervals, which may or may not represent important steps along the trajectory.
\hfill\break
\hfill\break

```{r}
round_track_time=function(dat, int, tol) {  #replacement for sett0() when wanting to only round some of the times
  
  for (i in 1:length(dat)) {
    tmp=matrix(NA,nrow(dat[[i]]),2)
    
    for (j in 1:nrow(dat[[i]])) {
      if (is.na(dat[[i]]$dt[j])) {
        tmp[j, 1:2]<- NA
      } else if (dat[[i]]$dt[j] > (int - tol) & dat[[i]]$dt[j] < (int + tol)) {
        tmp[j, 1:2]<- c(int, as.numeric(round(dat[[i]]$date[j], units = "hours")))
      } else {
        tmp[j, 1:2]<- c(dat[[i]]$dt[j], dat[[i]]$date[j])
      }
    }
    dat[[i]]$dt<- tmp[,1]
    dat[[i]]$date<- tmp[,2] %>% as.POSIXct(origin = '1970-01-01', tz = "UTC")
  }
  dat
}






dat.spdf<- dat
coordinates(dat.spdf)<- ~utmlong + utmlat
proj4string(dat.spdf)<- CRS("+init=epsg:32617")
dat.traj<- as.ltraj(xy = coordinates(dat.spdf), date = dat.spdf$ESTtime, id = dat.spdf$id)


#Fill in time gaps with NA vals (based on 1 h) and round w/in 5 min

refda<- dat[1,"ESTtime"]  #set reference time
dat.trajNA<- setNA(dat.traj, refda, 1, units = "hour")
dat.traj_round<- round_track_time(dat = dat.trajNA, int = 3600, tol = 5/60*3600)

#Remove NAs

dat.traj_round2<- na.omit.ltraj(dat.traj_round)


test.dat<- ld(dat.traj_round2)
tot.obs<- dat %>% group_by(id) %>% count();  tot.obs$id<- as.character(tot.obs$id)
hr_1<- test.dat %>% group_by(id) %>% filter(dt == 3600) %>% count(); hr_1$id<- as.character(hr_1$id)
hr_2<- test.dat %>% group_by(id) %>% filter(dt == 7200) %>% count(); hr_2$id<- as.character(hr_2$id)
hr_12<- test.dat %>% group_by(id) %>% filter(dt %% 3600 == 0 & dt <= 12*3600) %>% count()
hr_12$id<- as.character(hr_12$id)
hr_24<- test.dat %>% group_by(id) %>% filter(dt %% 3600 == 0 & dt <= 24*3600) %>% count()
hr_24$id<- as.character(hr_24$id)


summ.tab<- tot.obs %>% left_join(hr_1, by = "id") %>% left_join(hr_2, by = "id") %>%
  left_join(hr_12, by = "id") %>% left_join(hr_24, by = "id")

names(summ.tab)<- c("id","N","hr_1","hr_2","hr_12","hr_24")
summ.tab<- as.matrix(summ.tab)
summ.tab[is.na(summ.tab)]<- 0
summ.tab<- data.frame(summ.tab) %>% mutate_all(function(x) as.numeric(as.character(x)))
summ.tab$hr_2<- summ.tab$hr_2*2 + summ.tab$hr_1

#Make proportional to N per ID
summ.tab2<- matrix(NA,nrow(summ.tab), ncol(summ.tab)-1)

for (i in 1:nrow(summ.tab)) {
  summ.tab2[i,]<- as.numeric(summ.tab[i,-1]/summ.tab[i,2])  
}
summ.tab2<- cbind(id = summ.tab$id, summ.tab2) %>% data.frame()
colnames(summ.tab2)<- colnames(summ.tab)

summ.tab.long<- summ.tab2 %>% gather(key, value, -id)
summ.tab.long$id<- as.factor(summ.tab.long$id)
summ.tab.long$key<- factor(summ.tab.long$key, levels = c("hr_1","hr_2","hr_12","hr_24","N"))
summ.tab.long$type<- "Observed"






#Calculate the percentage of imputed data from whole dataset by max time interval (1, 2, 12, 24 hrs)

hr_1<- tot.obs; hr_1$n<- 0; hr_1$id<- as.character(hr_1$id)
hr_2<- test.dat %>% group_by(id) %>% filter(dt == 7200) %>% count(); hr_2$id<- as.character(hr_2$id)

grtr.12h <- function(dt) {  #cut traj when gap larger than 12 hrs
  return(dt > (1*3600*12))
}
dat.traj12h<- cutltraj(dat.traj, "grtr.12h(dt)", nextr = TRUE)
dat.traj12h_NA<- setNA(dat.traj12h, refda, 1, units = "hour")
dat.traj12h_round<- round_track_time(dat = dat.traj12h_NA, int = 3600, tol = 5/60*3600)
dat.traj12h_round<- ld(dat.traj12h_round)

hr_12<- dat.traj12h_round %>% group_by(id) %>% filter(is.na(x)) %>% count()
hr_12$id<- as.character(hr_12$id)

grtr.1d <- function(dt) {  #cut traj when gap larger than 1 day
  return(dt > (1*3600*24))
}

dat.traj1d<- cutltraj(dat.traj, "grtr.1d(dt)", nextr = TRUE)
dat.traj1d_NA<- setNA(dat.traj1d, refda, 1, units = "hour")
dat.traj1d_round<- round_track_time(dat = dat.traj1d_NA, int = 3600, tol = 5/60*3600)
dat.traj1d_round<- ld(dat.traj1d_round)

hr_24<- dat.traj1d_round %>% group_by(id) %>% filter(is.na(x)) %>% count()
hr_24$id<- as.character(hr_24$id)


summ.tab_imp<- tot.obs %>% left_join(hr_1, by = "id") %>% left_join(hr_2, by = "id") %>%
  left_join(hr_12, by = "id") %>% left_join(hr_24, by = "id")

names(summ.tab_imp)<- c("id","N","hr_1","hr_2","hr_12","hr_24")
summ.tab_imp<- as.matrix(summ.tab_imp)
summ.tab_imp[is.na(summ.tab_imp)]<- 0
summ.tab_imp<- data.frame(summ.tab_imp) %>% mutate_all(function(x) as.numeric(as.character(x)))

#Make proportional to N per ID
summ.tab_imp2<- matrix(NA,nrow(summ.tab_imp), ncol(summ.tab_imp)-1)

for (i in 1:nrow(summ.tab_imp)) {
  summ.tab_imp2[i,]<- as.numeric(summ.tab_imp[i,-1]/summ.tab_imp[i,2])
}
summ.tab_imp2<- cbind(id = summ.tab_imp$id, summ.tab_imp2) %>% data.frame()
colnames(summ.tab_imp2)<- colnames(summ.tab_imp)

summ.tab_imp.long<- summ.tab_imp2 %>% gather(key, value, -id)
summ.tab_imp.long$id<- as.factor(summ.tab_imp.long$id)
summ.tab_imp.long$key<- factor(summ.tab_imp.long$key, levels = c("hr_1","hr_2","hr_12","hr_24","N"))
summ.tab_imp.long$type<- "Imputed"


#Combine observed and imputed datasets
summ.tab.long_full<- rbind(summ.tab.long, summ.tab_imp.long)


```

```{r, fig.align='center', out.width='1.0\\linewidth', fig.pos='H', fig.cap="Observed and imputed data are scaled to the total number of observations by ID (N). N varies widely by ID, ranging from 158 to 9877 total observations. Dashed horizontal line denotes where counts are equal to N."}

### Plot
ggplot(summ.tab.long_full, aes(x=key, y=value, color = id)) +
  geom_hline(aes(yintercept = 1), size = 0.5, linetype = 2) +
  geom_point(size = 3) +
  theme_bw() +
  labs(x="\nMax Time Interval Included", y="Proportion of Total Observations\n") +
  theme(axis.title = element_text(size = 16), axis.text = element_text(size = 14),
        legend.title = element_text(size = 14)) +
  guides(color = guide_legend("ID")) + 
  scale_x_discrete(labels = c("1 hr","2 hrs","12 hrs","24 hrs","N")) +
  facet_wrap(~type, nrow = 2, scales = "free_y")
```


```{r, fig.align='center', out.width='1.0\\linewidth', fig.pos='H', fig.cap="Observed and imputed data are scaled to the total count of both observations and imputations for each max time interval. This does not take into account the original sample size of the observed dataset. Dashed horizontal line denotes where counts are equal to total count of observations and imputations."}


### Comparing only observed to imputed data

#Determine N per max time interval (obs + imputations)
summ.tab_full<- summ.tab[,3:6] + summ.tab_imp[,3:6]
summ.tab[,3:6]<- summ.tab[,3:6]/summ.tab_full
summ.tab<- summ.tab[,-2]


#Make proportional to N per ID
summ.tab2<- matrix(NA,nrow(summ.tab), ncol(summ.tab)-1)

for (i in 1:nrow(summ.tab)) {
  summ.tab2[i,]<- as.numeric(summ.tab[i,-1]/summ.tab[i,2])  
}
summ.tab2<- cbind(id = summ.tab$id, summ.tab2) %>% data.frame()
colnames(summ.tab2)<- colnames(summ.tab)

summ.tab.long<- summ.tab2 %>% gather(key, value, -id)
summ.tab.long$id<- as.factor(summ.tab.long$id)
summ.tab.long$key<- factor(summ.tab.long$key, levels = c("hr_1","hr_2","hr_12","hr_24"))
summ.tab.long$type<- "Observed"

summ.tab.long2<- summ.tab.long
summ.tab.long2$value<- 1-summ.tab.long2$value
summ.tab.long2$type<- "Imputed"

summ.tab.long_full<- rbind(summ.tab.long, summ.tab.long2)


### Plot
ggplot(summ.tab.long_full, aes(x=key, y=value, color = id)) +
  geom_hline(aes(yintercept = 1), size = 0.5, linetype = 2) +
  geom_point(size = 3) +
  theme_bw() +
  labs(x="\nMax Time Interval Included", y="Proportion of Dataset\n") +
  theme(axis.title = element_text(size = 16), axis.text = element_text(size = 14),
        legend.title = element_text(size = 14)) +
  guides(color = guide_legend("ID")) + 
  scale_x_discrete(labels = c("1 hr","2 hrs","12 hrs","24 hrs")) +
  facet_wrap(~type, nrow = 2)

```
\hfill\break

# Evaluating Imputed Tracks with High Variability

As seen in Figure 1, simulated tracks vary considerably in how meandering they are when missing data are imputed. This is likely a result of relocations that are not very far apart, but are separated by a large time interval (e.g. *dt* >> 1 h). The following figures investigate whether the clouds of simulated tracks observed for some IDs, such as **IDs 19 and 23**, are a result of very large time intervals. For comparison, I will also be displaying a plot for **ID 1**, which did not show this high level of variability among simulations. These plots display the mean trajectory resulting from 20 imputed paths. Red points denote the beginning and end of time intervals $\geq$ 15 h.

```{r, fig.align='center', out.width='1.0\\linewidth', fig.pos='H', fig.cap="Mean trajectory of 20 imputed paths for ID 19. Color ramp denotes standard deviation across all 20 trajectories in the x or y direction. Red points indicate beginning and end of large time intervals."}
## ID 19
dat19<- dat %>% filter(id == 19)
dat19_Ldt<- dat19 %>% mutate(dt = c(diff(ESTtime) %>% as.numeric(), NA)) #%>% filter(dt > 899)
ind<- which(dat19_Ldt$dt > 899)
dat19_Ldt<- dat19_Ldt[c(ind, ind+1),]
dat19_Ldt<- dat19_Ldt[order(dat19_Ldt$ESTtime),]
sim_tracks19<- sim_tracks_df %>% filter(id == 19)



x.19<- ggplot(data = sim_tracks19, aes(x=mu.x,y=mu.y,color=sd.x)) +
  geom_path(size = 0.5) +
  geom_point(data=dat19_Ldt, aes(x=utmlong, y=utmlat), color = "red", size = 1) +
  scale_color_viridis_c(option = "magma") +
  coord_equal() +
  theme_bw() +
  labs(x="Easting",y="Northing")

y.19<- ggplot(data = sim_tracks19, aes(x=mu.x,y=mu.y,color=sd.y)) +
  geom_path(size = 0.5) +
  geom_point(data=dat19_Ldt, aes(x=utmlong, y=utmlat), color = "red", size = 1) +
  scale_color_viridis_c(option = "magma") +
  coord_equal() +
  theme_bw() +
  labs(x="Easting",y="Northing")

plot_grid(x.19, y.19, labels = "", nrow = 1, align = "v")
```


```{r, fig.align='center', out.width='1.0\\linewidth', fig.pos='H', fig.cap="Mean trajectory of 20 imputed paths for ID 23. Color ramp denotes standard deviation across all 20 trajectories in the x or y direction. Red points indicate beginning and end of large time intervals."}
## ID 23
dat23<- dat %>% filter(id == 23)
dat23_Ldt<- dat23 %>% mutate(dt = c(diff(ESTtime) %>% as.numeric(), NA)) #%>% filter(dt > 899)
ind<- which(dat23_Ldt$dt > 899)
dat23_Ldt<- dat23_Ldt[c(ind, ind+1),]
dat23_Ldt<- dat23_Ldt[order(dat23_Ldt$ESTtime),]
sim_tracks23<- sim_tracks_df %>% filter(id == 23)



x.23<- ggplot(data = sim_tracks23, aes(x=mu.x,y=mu.y,color=sd.x)) +
  geom_path(size = 0.5) +
  geom_point(data=dat23_Ldt, aes(x=utmlong, y=utmlat), color = "red", size = 1) +
  scale_color_viridis_c(option = "magma") +
  coord_equal() +
  theme_bw() +
  labs(x="Easting",y="Northing")

y.23<- ggplot(data = sim_tracks23, aes(x=mu.x,y=mu.y,color=sd.y)) +
  geom_path(size = 0.5) +
  geom_point(data=dat23_Ldt, aes(x=utmlong, y=utmlat), color = "red", size = 1) +
  scale_color_viridis_c(option = "magma") +
  coord_equal() +
  theme_bw() +
  labs(x="Easting",y="Northing")

plot_grid(x.23, y.23, labels = "", nrow = 1, align = "v")
```

```{r, fig.align='center', out.width='1.0\\linewidth', fig.pos='H', fig.cap="Mean trajectory of 20 imputed paths for ID 1. Color ramp denotes standard deviation across all 20 trajectories in the x or y direction. Red points indicate beginning and end of large time intervals."}
## ID 1
dat1<- dat %>% filter(id == 1)
dat1_Ldt<- dat1 %>% mutate(dt = c(diff(ESTtime) %>% as.numeric(), NA)) #%>% filter(dt > 899)
ind<- which(dat1_Ldt$dt > 899)
dat1_Ldt<- dat1_Ldt[c(ind, ind+1),]
dat1_Ldt<- dat1_Ldt[order(dat1_Ldt$ESTtime),]
sim_tracks1<- sim_tracks_df %>% filter(id == 1)



x.1<- ggplot(data = sim_tracks1, aes(x=mu.x,y=mu.y,color=sd.x)) +
  geom_path(size = 0.5) +
  geom_point(data=dat1_Ldt, aes(x=utmlong, y=utmlat), color = "red", size = 1) +
  scale_color_viridis_c(option = "magma") +
  coord_equal() +
  theme_bw() +
  labs(x="Easting",y="Northing")

y.1<- ggplot(data = sim_tracks1, aes(x=mu.x,y=mu.y,color=sd.y)) +
  geom_path(size = 0.5) +
  geom_point(data=dat1_Ldt, aes(x=utmlong, y=utmlat), color = "red", size = 1) +
  scale_color_viridis_c(option = "magma") +
  coord_equal() +
  theme_bw() +
  labs(x="Easting",y="Northing")

plot_grid(x.1, y.1, labels = "", nrow = 1, align = "v")
```
\hfill\break

Given these comparisons, it appears that these very large time gaps appear to be the primary cause of high variability in the imputed tracks. Although there are some large time intervals for **ID 1**, these imputed tracks appear relatively straight. This is likely a result of the large distances covered for this ID, which would necessitate the track be relatively straight to reach the next observed location.
\hfill\break
\hfill\break

# Conclusions

These exploratory analyses demonstrate the utility of process imputation for recovering observations at other time intervals than the primary one (1 h), but also identify some shortcomings for the datasets where time intervals may be highly irregular. For the snail kite telemetry data, the use of a maximum time interval of 12 h would result in imputed data comprising nearly 50% of the total dataset and approximately 75% if set to 24 h. Therefore, the use of a 2 h maximum time interval for imputation would provide the best tradeoff for a larger sample size while minimizing the proportion of imputed data in further analyses. However, there is still the problem of the remaining observations that will be excluded from further analyses. These excluded data represent 30% of the whole dataset on average (range: 14 - 67%). Cutting the trajectories above a threshold of some time interval would not necessarily help if imputation is already conducted on a relatively short max time interval. For the snail kite data, this is because many of these irregularly sampled data are sampled at intervals shorter than 1 h (~ 4200 observations; 8% of entire dataset). While this does not necessarily need to be addressed, it would need to be recognized as a limitation of our method if using telemetry data that are highly irregular.